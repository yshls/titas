import { useState, useRef, useEffect, useCallback } from 'react';
import toast from 'react-hot-toast';
import { usePracticeStore } from '@/store/practiceStore';
import { useSpeechRecognition } from '@/utils/useSpeechRecognition';
import { useTTS } from '@/utils/useTTS';
import { checkWordDiff } from '@/utils/diffChecker';

export function useUserInput() {
  // --- Ïä§ÌÜ†Ïñ¥ ÏÉÅÌÉú ÏÑ†ÌÉù
  const status = usePracticeStore((state) => state.status);
  const currentLineIndex = usePracticeStore((state) => state.currentLineIndex);
  const currentLine = usePracticeStore(
    (state) => state.lines[state.currentLineIndex],
  );
  const userSpeakerId = usePracticeStore((state) => state.userSpeakerId);
  const addUserInput = usePracticeStore((state) => state.addUserInput);
  const addUserAudio = usePracticeStore((state) => state.addUserAudio);
  const advanceLine = usePracticeStore((state) => state.advanceLine);

  const isMyTurn =
    status === 'active' && currentLine?.speakerId === userSpeakerId; // --- ÎÇ¥Î∂Ä UI ÏÉÅÌÉú

  const [inputMode, setInputMode] = useState<'mic' | 'keyboard'>('mic');
  const [typedInput, setTypedInput] = useState('');
  const [isProcessing, setIsProcessing] = useState(false);
  const [mediaStream, setMediaStream] = useState<MediaStream | null>(null); // --- Ïô∏Î∂Ä ÌõÖ

  const {
    transcript,
    isListening,
    startListening,
    stopListening,
    clearTranscript,
    permissionStatus,
  } = useSpeechRecognition();

  const { isSpeaking } = useTTS(); // --- Ref

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const hasProcessedCurrentLine = useRef(false); // ÎùºÏù∏ Î≥ÄÍ≤Ω Ïãú Ï≤òÎ¶¨ ÌîåÎûòÍ∑∏ Ï¥àÍ∏∞Ìôî

  useEffect(() => {
    hasProcessedCurrentLine.current = false;
  }, [currentLineIndex]); // ÎîîÎ≤ÑÍπÖ: transcript Î≥ÄÌôî Ï∂îÏ†Å

  useEffect(() => {
    console.log('üîµ [useUserInput] transcript changed:', transcript);
    console.log('üîµ [useUserInput] isListening:', isListening);
    console.log('üîµ [useUserInput] isMyTurn:', isMyTurn);
  }, [transcript, isListening, isMyTurn]); // --- ÌïµÏã¨ Î°úÏßÅ: ÎÖπÏùå Î∞è ÏùåÏÑ± Ïù∏Ïãù Ï§ëÏßÄ

  const stopRecordingAndListening = useCallback(async () => {
    return new Promise<void>((resolve) => {
      let resolved = false;

      const cleanup = () => {
        if (!resolved) {
          resolved = true;
          if (mediaStream) {
            mediaStream.getTracks().forEach((track) => track.stop());
            setMediaStream(null);
          }
          resolve();
        }
      };

      if (mediaRecorderRef.current?.state === 'recording') {
        mediaRecorderRef.current.onstop = () => {
          const blob = new Blob(audioChunksRef.current, {
            type: 'audio/webm',
          });
          const url = URL.createObjectURL(blob);
          addUserAudio(currentLineIndex, url);
          cleanup();
        };
        mediaRecorderRef.current.stop();
      } else {
        cleanup();
      }

      if (isListening) {
        stopListening();
      }

      setTimeout(() => cleanup(), 500);
    });
  }, [isListening, stopListening, mediaStream, addUserAudio, currentLineIndex]); // --- ÌïµÏã¨ Î°úÏßÅ: ÏûÖÎ†• Ï≤òÎ¶¨ Î∞è Îã§Ïùå ÎùºÏù∏ ÏßÑÌñâ

  const processAndAdvance = useCallback(
    async (text: string) => {
      if (!currentLine || isProcessing || hasProcessedCurrentLine.current) {
        console.log('‚ö†Ô∏è [processAndAdvance] Skipped:', {
          hasCurrentLine: !!currentLine,
          isProcessing,
          hasProcessed: hasProcessedCurrentLine.current,
        });
        return;
      }

      if (!text.trim()) {
        toast.error("Oops! I didn't catch that. Could you please try again?");
        await stopRecordingAndListening();
        return;
      }

      console.log('‚úÖ [processAndAdvance] Starting to process:', text);
      setIsProcessing(true);
      hasProcessedCurrentLine.current = true;
      await stopRecordingAndListening();

      const originalText = currentLine.originalLine
        .replace(/[^\w\s']/g, '')
        .toLowerCase();
      const processedInput = text
        .trim()
        .replace(/[^\w\s']/g, '')
        .toLowerCase();

      if (!processedInput) {
        setIsProcessing(false);
        return;
      }

      console.log('üìä [processAndAdvance] Comparing:', {
        original: originalText,
        spoken: processedInput,
      });

      const diff = checkWordDiff(originalText, processedInput);
      console.log('üìä [processAndAdvance] Diff result:', diff);

      addUserInput(currentLineIndex, text, diff);
      clearTranscript();

      setTimeout(() => {
        advanceLine();
        setIsProcessing(false);
      }, 2000);
    },
    [
      currentLine,
      isProcessing,
      stopRecordingAndListening,
      addUserInput,
      currentLineIndex,
      advanceLine,
      clearTranscript,
    ],
  ); // --- ÏûêÎèô Ï†úÏ∂ú: transcriptÍ∞Ä ÏôÑÏÑ±ÎêòÎ©¥ ÏûêÎèôÏúºÎ°ú Ï≤òÎ¶¨

  useEffect(() => {
    console.log('üü¢ [Auto-submit check]', {
      hasTranscript: !!transcript,
      transcriptLength: transcript.length,
      notListening: !isListening,
      isMyTurn,
      hasProcessed: hasProcessedCurrentLine.current,
    }); // transcriptÍ∞Ä ÏûàÍ≥†, ÏùåÏÑ± Ïù∏ÏãùÏù¥ ÎÅùÎÇ¨Í≥†, ÎÇ¥ Ï∞®Î°ÄÏù¥Í≥†, ÏïÑÏßÅ Ï≤òÎ¶¨ Ïïà ÌñàÏùÑ Îïå

    if (
      transcript &&
      transcript.trim().length > 0 &&
      !isListening &&
      isMyTurn &&
      !hasProcessedCurrentLine.current
    ) {
      console.log('‚úÖ [Auto-submit] Conditions met! Processing...');
      processAndAdvance(transcript);
    }
  }, [transcript, isListening, isMyTurn, processAndAdvance]); // --- ÎÖπÏùå ÏãúÏûë

  const startRecording = useCallback(async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    setMediaStream(stream);

    const mimeType = MediaRecorder.isTypeSupported('audio/webm')
      ? 'audio/webm'
      : 'audio/mp4';

    const recorder = new MediaRecorder(stream, { mimeType });
    mediaRecorderRef.current = recorder;
    audioChunksRef.current = [];
    recorder.ondataavailable = (e) => {
      if (e.data.size > 0) audioChunksRef.current.push(e.data);
    };
    recorder.onstop = () => {
      const blob = new Blob(audioChunksRef.current, { type: mimeType });
      const url = URL.createObjectURL(blob);
      addUserAudio(currentLineIndex, url);
    };
    recorder.start();
    return true;
  }, [addUserAudio, currentLineIndex]); // --- Í∂åÌïú Í¥ÄÎ¶¨

  const [isPermissionRequestPending, setIsPermissionRequestPending] =
    useState(false);

  const requestPermission = useCallback(async () => {
    console.log('[useUserInput] Requesting microphone permission...');
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log('[useUserInput] Permission granted.');
      stream.getTracks().forEach((track) => track.stop());
    } catch (error) {
      console.error('[useUserInput] Permission denied.', error);
      toast.error('Microphone access is required to use voice input.');
    }
  }, []); // --- ÏùåÏÑ± Ïù∏Ïãù ÏãúÏûë

  const startRecognition = useCallback(async () => {
    if (!isMyTurn) return;
    setTypedInput('');
    const recordingStarted = await startRecording();
    if (recordingStarted) {
      startListening();
    }
  }, [isMyTurn, startRecording, startListening]); // Í∂åÌïú ÏäπÏù∏ ÌõÑ ÏûêÎèô ÏãúÏûë

  useEffect(() => {
    if (permissionStatus === 'granted' && isPermissionRequestPending) {
      startRecognition();
      setIsPermissionRequestPending(false);
    }
  }, [permissionStatus, isPermissionRequestPending, startRecognition]); // --- Ïù¥Î≤§Ìä∏ Ìï∏Îì§Îü¨: ÎßàÏù¥ÌÅ¨ Î≤ÑÌäº ÌÅ¥Î¶≠

  const handleMicClick = async () => {
    if (isSpeaking) return;

    if (isListening) {
      console.log('üõë [handleMicClick] Stopping...');
      stopRecordingAndListening();
      return;
    }

    if (permissionStatus === 'denied') {
      toast.error(
        'Microphone access is denied. Please enable it in your browser settings.',
      );
      return;
    }

    if (permissionStatus === 'prompt') {
      setIsPermissionRequestPending(true);
      await requestPermission();
      return;
    }

    if (permissionStatus === 'granted') {
      console.log('üé§ [handleMicClick] Starting recognition...');
      startRecognition();
    }
  }; // --- Ïù¥Î≤§Ìä∏ Ìï∏Îì§Îü¨: ÌÇ§Î≥¥Îìú Ï†úÏ∂ú

  const handleKeyboardSubmit = () => {
    if (typedInput.trim()) {
      processAndAdvance(typedInput);
      setTypedInput('');
    }
  };

  return {
    inputMode,
    setInputMode,
    typedInput,
    setTypedInput,
    transcript,
    isListening,
    isProcessing,
    isSpeaking,
    mediaStream,
    handleMicClick,
    handleKeyboardSubmit,
    stopRecordingAndListening,
  };
}
