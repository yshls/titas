import { useState, useEffect, useRef, useCallback } from 'react';

// Web Speech API ë¸Œë¼ìš°ì € í˜¸í™˜ì„± íƒ€ì… ì„ ì–¸
declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
  }
}

const SpeechRecognition =
  window.SpeechRecognition || window.webkitSpeechRecognition;

// API ë¯¸ì§€ì› ë¸Œë¼ìš°ì € ê²½ê³ 
if (!SpeechRecognition && typeof window !== 'undefined') {
  console.warn(
    'Your browser does not support the Web Speech API. Please use Chrome.',
  );
}

// âœ… ëª¨ë°”ì¼ ê°ì§€
const isMobile =
  typeof window !== 'undefined' &&
  /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);

// ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ í•¨ìˆ˜
const requestMicrophonePermission = async (
  setPermissionStatus: (status: PermissionState) => void,
): Promise<boolean> => {
  console.log('[SpeechRecognition] Requesting microphone permission...');
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    console.log('[SpeechRecognition] Permission granted.');
    setPermissionStatus('granted');
    stream.getTracks().forEach((track) => track.stop());
    return true;
  } catch (error) {
    console.error('[SpeechRecognition] Permission denied.', error);
    setPermissionStatus('denied');
    return false;
  }
};

export function useSpeechRecognition() {
  const [transcript, setTranscript] = useState('');
  const [isListening, setIsListening] = useState(false);
  const [permissionStatus, setPermissionStatus] =
    useState<PermissionState>('prompt');

  const recognitionRef = useRef<any>(null);
  const silenceTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null);

  // ì¹¨ë¬µ íƒ€ì´ë¨¸ ì •ë¦¬ í•¨ìˆ˜
  const clearSilenceTimer = useCallback(() => {
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
  }, []);

  // ìŒì„± ì¸ì‹ ì¤‘ì§€ í•¨ìˆ˜
  const stopRecognition = useCallback(() => {
    console.log('[STOP] Stopping recognition...');
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
      } catch (e) {
        console.error('[STOP] Error stopping recognition:', e);
      }
    }
    setIsListening(false);
    clearSilenceTimer();
  }, [clearSilenceTimer]);

  // ìŒì„± ì¸ì‹ ì´ˆê¸°í™”
  useEffect(() => {
    // ë¸Œë¼ìš°ì € í˜¸í™˜ì„± ì²´í¬
    if (!navigator.mediaDevices) {
      console.error('[SpeechRecognition] navigator.mediaDevices not available');
      setPermissionStatus('denied');
      return;
    }

    // ê¶Œí•œ ìš”ì²­
    requestMicrophonePermission(setPermissionStatus);

    // Web Speech API ì§€ì› ì²´í¬
    if (!SpeechRecognition) {
      console.log('[SpeechRecognition] API not supported');
      return;
    }

    // Recognition ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    const recognition = new SpeechRecognition();
    recognition.lang = 'en-US';
    recognition.interimResults = true;

    // âœ… ëª¨ë°”ì¼ì—ì„œëŠ” continuous false
    recognition.continuous = isMobile ? false : true;
    recognition.maxAlternatives = 1;

    console.log(
      `ğŸ”§ [CONFIG] isMobile: ${isMobile}, continuous: ${recognition.continuous}`,
    );

    // ë””ë²„ê¹…: ì˜¤ë””ì˜¤ ì´ë²¤íŠ¸
    recognition.onaudiostart = () => {
      console.log('ğŸ™ï¸ [onaudiostart] ë§ˆì´í¬ ì…ë ¥ ê°ì§€ ì‹œì‘!');
    };

    recognition.onaudioend = () => {
      console.log('ğŸ™ï¸ [onaudioend] ë§ˆì´í¬ ì…ë ¥ ì¢…ë£Œ');
    };

    recognition.onsoundstart = () => {
      console.log('ğŸ”Š [onsoundstart] ì†Œë¦¬ ê°ì§€ë¨!');
    };

    recognition.onsoundend = () => {
      console.log('ğŸ”‡ [onsoundend] ì†Œë¦¬ ê°ì§€ ì¢…ë£Œ');
    };

    recognition.onspeechstart = () => {
      console.log('ğŸ—£ï¸ [onspeechstart] ë§ì†Œë¦¬ ê°ì§€ë¨!');
    };

    recognition.onspeechend = () => {
      console.log('ğŸ¤ [onspeechend] ë§ì†Œë¦¬ ëë‚¨');
    };

    // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬: ì‹œì‘
    recognition.onstart = () => {
      console.log('>>> [onstart] Recognition started');
      setIsListening(true);
    };

    // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬: ê²°ê³¼
    recognition.onresult = (event: any) => {
      console.log('>>> [onresult] Result received');
      console.log('>>> [onresult] Results length:', event.results.length);

      // ê¸°ì¡´ ì¹¨ë¬µ íƒ€ì´ë¨¸ ì·¨ì†Œ
      clearSilenceTimer();

      let interimText = '';
      let finalText = '';

      // âœ… ëª¨ë“  ê²°ê³¼ë¥¼ ì²˜ë¦¬ (resultIndexë¶€í„°ê°€ ì•„ë‹Œ ì „ì²´)
      for (let i = 0; i < event.results.length; i++) {
        const text = event.results[i][0].transcript;
        const confidence = event.results[i][0].confidence;
        console.log(
          `>>> [onresult] Result[${i}]: "${text}" (isFinal: ${event.results[i].isFinal}, confidence: ${confidence})`,
        );

        if (event.results[i].isFinal) {
          finalText += text + ' ';
        } else {
          interimText += text;
        }
      }

      // ì„ì‹œ ê²°ê³¼ ë¡œê¹…
      if (interimText) {
        console.log('[Interim]:', interimText);
        // âœ… ëª¨ë°”ì¼ì—ì„œëŠ” interimë„ transcriptì— ë°˜ì˜
        if (isMobile) {
          setTranscript(interimText);
        }
      }

      // í™•ì • ê²°ê³¼ ì²˜ë¦¬
      if (finalText) {
        console.log('[Final]:', finalText);
        console.log('[Final] Setting transcript...');
        setTranscript((prev) => {
          const newValue = prev + finalText;
          console.log('[Final] New transcript value:', newValue);
          return newValue;
        });

        // âœ… ëª¨ë°”ì¼: 1ì´ˆ, PC: 2ì´ˆ
        const timeout = isMobile ? 1000 : 2000;
        silenceTimerRef.current = setTimeout(() => {
          console.log(`[Auto-stop] ${timeout / 1000}ì´ˆ ì¹¨ë¬µ ê°ì§€ â†’ ìë™ ì¤‘ì§€`);
          stopRecognition();
        }, timeout);
      }
    };

    // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬: ì—ëŸ¬
    recognition.onerror = (event: any) => {
      console.error(`!!! [onerror] ${event.error}`, event);

      // no-speech ì—ëŸ¬: ëª¨ë°”ì¼ì—ì„œëŠ” ê·¸ëƒ¥ ì¤‘ì§€
      if (event.error === 'no-speech') {
        console.log('[Info] No speech detected');
        if (isMobile) {
          // âœ… ëª¨ë°”ì¼: interim ê²°ê³¼ë¼ë„ ìˆìœ¼ë©´ ì‚¬ìš©
          console.log('[Mobile] Checking interim results...');
          stopRecognition();
        }
        return;
      }

      // ê¶Œí•œ ê±°ë¶€ ì—ëŸ¬ ì²˜ë¦¬
      if (
        event.error === 'not-allowed' ||
        event.error === 'service-not-allowed'
      ) {
        console.log('[Error] Microphone permission denied');
        setPermissionStatus('denied');
      }

      // aborted ì—ëŸ¬ëŠ” ìˆ˜ë™ ì¤‘ì§€ì´ë¯€ë¡œ ë¬´ì‹œ
      if (event.error === 'aborted') {
        console.log('[Info] Recognition aborted (manual stop)');
        return;
      }

      setIsListening(false);
    };

    // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬: ì¢…ë£Œ
    recognition.onend = () => {
      console.log('>>> [onend] Recognition ended');
      console.log('>>> [onend] Final transcript:', transcript);
      setIsListening(false);
      clearSilenceTimer();

      // âœ… ëª¨ë°”ì¼: continuous falseë¼ì„œ ìë™ ì¬ì‹œì‘ ë°©ì§€
      if (isMobile) {
        console.log('[Mobile] Recognition ended naturally');
      }
    };

    recognitionRef.current = recognition;

    // í´ë¦°ì—…
    return () => {
      clearSilenceTimer();
      if (recognitionRef.current) {
        try {
          recognitionRef.current.abort();
        } catch (e) {
          // ì´ë¯¸ ì¢…ë£Œëœ ê²½ìš° ë¬´ì‹œ
        }
      }
    };
  }, []);

  // ìŒì„± ì¸ì‹ ì‹œì‘
  const startListening = useCallback(async () => {
    console.log('[START] Attempting to start recognition...');
    console.log('[START] isMobile:', isMobile);

    // ê¶Œí•œ ì²´í¬
    if (permissionStatus === 'prompt') {
      console.log('[START] Requesting permission first...');
      const granted = await requestMicrophonePermission(setPermissionStatus);
      if (!granted) {
        console.warn('[START] Permission denied, cannot start');
        return;
      }
    }

    if (permissionStatus !== 'granted') {
      console.warn('[START] Permission not granted');
      return;
    }

    // ì´ë¯¸ ë“£ê³  ìˆìœ¼ë©´ ì¤‘ë³µ ì‹œì‘ ë°©ì§€
    if (isListening) {
      console.warn('[START] Already listening');
      return;
    }

    // Recognition ì‹œì‘
    if (recognitionRef.current) {
      try {
        setTranscript(''); // ì´ì „ transcript ì´ˆê¸°í™”
        console.log('[START] Starting recognition...');
        recognitionRef.current.start();
      } catch (e) {
        console.error('[START] Failed to start:', e);
      }
    } else {
      console.error('[START] Recognition instance not initialized');
    }
  }, [permissionStatus, isListening]);

  // ìŒì„± ì¸ì‹ ì¤‘ì§€
  const stopListening = useCallback(() => {
    stopRecognition();
  }, [stopRecognition]);

  // Transcript ì´ˆê¸°í™”
  const clearTranscript = useCallback(() => {
    console.log('[CLEAR] Clearing transcript');
    setTranscript('');
  }, []);

  // ê¶Œí•œ ì¬ìš”ì²­
  const requestPermission = useCallback(async () => {
    return await requestMicrophonePermission(setPermissionStatus);
  }, []);

  return {
    transcript,
    isListening,
    permissionStatus,
    startListening,
    stopListening,
    clearTranscript,
    requestPermission,
  };
}
